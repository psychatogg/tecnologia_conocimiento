---
title: "Tarea_8"
author: "Juliana Quirós, Alberto"
geometry: margin=1cm
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 10), tidy = TRUE)
```

El presente código prepara los datos de la base "mt cars" para un análisis de k-medias. \ Defino inicialmente 5 clusters basándome en la estabilidad de  la variabilidad intracluster del gráfico de codo y la facilidad de visualización de dichos clusteres. \
La función kmeans() realizará las siguientes operaciones:  \
1.  Toma los puntos más lejanos y, comparando las distancias de cada observación con su extremo, asigna a los clusters por cercanías. \
2. Traza un centroide a partir de la suma vectorial de los ejemplares asignados a cada cluster \
3. Como dichos centroides son los nuevos representantes de cada categoría, será necesario recalcular las distancias de cada ejemplar y reasignar si es necesario.
```{r,warning=FALSE}
#http://www.sthda.com/english/wiki/factoextra-r-package-easy-multivariate-data-analyses-and-elegant-


library(factoextra) 

df <- scale(mtcars) # Scaling the data 
head(df, n = 3) 

 

set.seed(123) 
elbow <- fviz_nbclust(df, kmeans, method = "wss", k.max = 10) ## elbow
elbow
km.res <- kmeans(df, 4, nstart = 25) 

 

print(km.res) 

 

km.res$cluster 

 

head(km.res$cluster, 4) 

 

# Cluster size 
km.res$size 

 

# Cluster means 
km.res$centers 

 

fviz_cluster(km.res, data = df, 
             palette = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07", 
"#00AFBB","#2E9FDF","#2E9FDF","#2E9FDF","#2E9FDF"), 
             ggtheme = theme_minimal(), 
             main = "Partitioning Clustering Plot" 
) 

```

Dado que contamos con 11 variables y el procedimiento utiliza reducción de dimensiones por ACP, podemos profundizar en la estructura de dichos factores resultantes:

```{r}
library(hornpa)
pca.res <- prcomp(df,rank= 2) 
pca.res$rotation
pca.var =pca.res$sdev ^2
## Comparo datos simulados con los autovalores del dataset
simulacion <- hornpa(k = 11, size = 50, reps = 500, seed = 123)
pca.var
```





Probamos un número superior de iteraciones
```{r}

set.seed(123) 
km.res <- kmeans(df, 4, nstart = 250) ## Aumento número de iteraciones

 

print(km.res) 

 

km.res$cluster 

 

head(km.res$cluster, 4) 

 

# Cluster size 
km.res$size 

 

# Cluster means 
km.res$centers 

 

fviz_cluster(km.res, data = df, 
             palette = c("#00AFBB","#2E9FDF", "#E7B800", "#FC4E07", 
"#00AFBB","#2E9FDF","#2E9FDF","#2E9FDF","#2E9FDF"), 
             ggtheme = theme_minimal(), 
             main = "Partitioning Clustering Plot" 
) 
```

Comprobamos que en 25 iteraciones ya se llegó a la solución óptima, dado que los ejemplares ya se encontraban lo suficientemente diferenciados.
